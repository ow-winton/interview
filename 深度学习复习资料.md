# 深度学习相关资料

## 1. XGBOOST lightGBM CATBOOST (三大主流的Boosting框架)
- XGboost ( eXtream Gradient Boosting ) 极端梯度提升。  本质上就是集成多个树。 特点是XGboost是串行的boost。
- RF（随机森林）随机森林是并行的。

三者的主要区别：
- Xgboost不支持自动编码，需要手动label encoding, 而且对于大数据集和高纬特征效率较差，不如LightGBM 快
- LightGBM 专注训练效率，采用了leaf-wise的生长策略




## 2. GBDT 
GBDT 是决策树， 起初没有损失函数，主要有三个任务，分类，回归和回归变分类。 分类是通过： 信息增益和基尼系数来区分数据差异最大的内容。 回归和线性回归一样，通过loss function来最小化均方误差。 还有就是类似逻辑回归，把结果归到0~1区间内，就变成分类了。 

从Gradient和Boost的角度区分的话。 Gradient就是针对loss进行梯度下降， Boost就是集成学习的思路，用很多小模型提升鲁棒性。 比如随机森林用的Bagging (Boostrap Aggregating) 有放回的随机采样构成了很多数据集

Dart模型： 在决策树基础上加了个drop out

## 3. TRANSFORMER

## 4. CLIP BLIP BLIP2 ALBUF

## 5. Leaf-wise和Level-wise
- Level-wise  广度优先构建树。每一层的所有节点同时分裂，再进入下一层, 常见于：XGBoost, Catboost的也类似这种
- Leaf-wise  每次只选择 一个最优叶子节点 来分裂 —— 通常是“带来最大信息增益（Gain）”的那个。 常见于：LightGBM

如果你对过拟合非常敏感（如样本量小），Level-wise 更稳健。
如果你追求极致性能（如大样本、稀疏特征、低噪声），Leaf-wise 更高效、更准确，但要小心过拟合，需调好 max_depth 和 min_data_in_leaf。

