# 深度学习相关资料

## 1. XGBOOST lightGBM CATBOOST (三大主流的Boosting框架)
- XGboost ( eXtream Gradient Boosting ) 极端梯度提升。  本质上就是集成多个树。 特点是XGboost是串行的boost。
- RF（随机森林）随机森林是并行的。

三者的主要区别：
- Xgboost不支持自动编码，需要手动label encoding, 而且对于大数据集和高纬特征效率较差，不如LightGBM 快
- LightGBM 专注训练效率，采用了leaf-wise的生长策略，而且支持直方图算法（xgboost后续才加入支持）：直方图算法的意思是将连续特征值转换成了k个桶，然后基于桶的统计信息来找最佳分裂点（适用于处理大规模数据）。 


| 特征         | XGBoost                   | LightGBM           | CatBoost          |
| ---------- | ------------------------- | ------------------ | ----------------- |
| 是否支持直方图算法  | ✅ 支持（后加入）                 | ✅ 原生支持             | ✅ 原生支持            |
| 默认是否启用     | ❌ 否（默认用精确分裂）              | ✅ 是（默认用直方图）        | ✅ 是               |
| 特点         | 稍慢、但支持更多优化                | 高效、速度快             | 高效、自动处理类别特征       |
| 特征预处理（离散化） | 可手动 binning 或启用 histogram | 自动、默认 bin 为 255 个桶 | 自动离散数值 + 高效编码类别变量 |


## 2. GBDT 
GBDT 是决策树， 起初没有损失函数，主要有三个任务，分类，回归和回归变分类。 分类是通过： 信息增益和基尼系数来区分数据差异最大的内容。 回归和线性回归一样，通过loss function来最小化均方误差。 还有就是类似逻辑回归，把结果归到0~1区间内，就变成分类了。 

从Gradient和Boost的角度区分的话。 Gradient就是针对loss进行梯度下降， Boost就是集成学习的思路，用很多小模型提升鲁棒性。 比如随机森林用的Bagging (Boostrap Aggregating) 有放回的随机采样构成了很多数据集

Dart模型： 在决策树基础上加了个drop out

## 3. TRANSFORMER

## 4. CLIP BLIP BLIP2 ALBUF

## 5. Leaf-wise和Level-wise
- Level-wise  广度优先构建树。每一层的所有节点同时分裂，再进入下一层, 常见于：XGBoost, Catboost的也类似这种
- Leaf-wise  每次只选择 一个最优叶子节点 来分裂 —— 通常是“带来最大信息增益（Gain）”的那个。 常见于：LightGBM

如果你对过拟合非常敏感（如样本量小），Level-wise 更稳健。
如果你追求极致性能（如大样本、稀疏特征、低噪声），Leaf-wise 更高效、更准确，但要小心过拟合，需调好 max_depth 和 min_data_in_leaf。

