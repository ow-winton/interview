# 深度学习相关资料

## 1. XGBOOST lightGBM CATBOOST (三大主流的Boosting框架)
- XGboost ( eXtream Gradient Boosting ) 极端梯度提升。  本质上就是集成多个树。 特点是XGboost是串行的boost。
- RF（随机森林）随机森林是并行的。


| 项目       | XGBoost   | LightGBM            | CatBoost    |
| -------- | --------- | ------------------- | ----------- |
| 训练速度     | 中等        | ⭐最快                 | ⭐⭐偏慢        |
| 精度表现     | ⭐⭐⭐⭐稳定    | ⭐⭐⭐⭐略好              | ⭐⭐⭐⭐对类别最有优势 |
| 缺失值处理    | ✅ 自动支持    | ✅ 自动支持              | ✅ 自动支持      |
| 类别特征处理   | ❌ 需人工预处理  | ✅ 支持 label encoding | ✅✅✅ 无需预处理   |
| 并行/分布式支持 | ✅ 多线程+分布式 | ✅ 支持（非常高效）          | ✅ 支持        |
| 调参难度     | ⭐⭐⭐ 高     | ⭐⭐ 中等               | ⭐ 最低        |


| 场景               | 推荐算法         | 原因说明            |
| ---------------- | ------------ | --------------- |
| 中小型数据 + 熟悉调参     | **XGBoost**  | 表现稳定，工具链成熟      |
| 超大规模、稀疏特征、训练效率优先 | **LightGBM** | 训练效率极高，资源占用低    |
| 类别特征复杂、少调参       | **CatBoost** | 自动编码最强，预测快，稳定性好 |

三者的主要区别：
- Xgboost不支持自动编码，需要手动label encoding, 而且对于大数据集和高纬特征效率较差，不如LightGBM 快
- LightGBM 专注训练效率，采用了leaf-wise的生长策略，而且支持直方图算法（xgboost后续才加入支持）：直方图算法的意思是将连续特征值转换成了k个桶，然后基于桶的统计信息来找最佳分裂点（适用于处理大规模数据）。 


| 特征         | XGBoost                   | LightGBM           | CatBoost          |
| ---------- | ------------------------- | ------------------ | ----------------- |
| 是否支持直方图算法  | ✅ 支持（后加入）                 | ✅ 原生支持             | ✅ 原生支持            |
| 默认是否启用     | ❌ 否（默认用精确分裂）              | ✅ 是（默认用直方图）        | ✅ 是               |
| 特点         | 稍慢、但支持更多优化                | 高效、速度快             | 高效、自动处理类别特征       |
| 特征预处理（离散化） | 可手动 binning 或启用 histogram | 自动、默认 bin 为 255 个桶 | 自动离散数值 + 高效编码类别变量 |


## 2. GBDT 
GBDT 是决策树， 起初没有损失函数，主要有三个任务，分类，回归和回归变分类。 分类是通过： 信息增益和基尼系数来区分数据差异最大的内容。 回归和线性回归一样，通过loss function来最小化均方误差。 还有就是类似逻辑回归，把结果归到0~1区间内，就变成分类了。 

从Gradient和Boost的角度区分的话。 Gradient就是针对loss进行梯度下降， Boost就是集成学习的思路，用很多小模型提升鲁棒性。 比如随机森林用的Bagging (Boostrap Aggregating) 有放回的随机采样构成了很多数据集

Dart模型： 在决策树基础上加了个drop out

## 3. TRANSFORMER
结构就是ENCODER -- DECODER架构， 但是实际应用可能不会两个都用。
BERT只用了ENCODER， GPT只用了decoder。

encoder一般是多个一样的层堆叠的，每层有两个子结构（多头自注意力机制和前馈神经网络），子层上添加了残差链接和层归一化。
decoder的话也是多个层堆叠，每层三个子结构 Masked Multi-Head Self Attention （防止模型看到未来） Encoder-Decoder Attention（关注 Encoder 输出）以及FFN， 同样也有残差连接和归一化。
核心机制就是
- 自注意力或者多头注意力，自注意力是softmax(Q×K的转置/ 根号下dk)*V, 本质上其实是在变换权重矩阵
- 多头注意力
- feed forward network
- 位置编码，用sin cos来编码位置加到内容里

## 4. CLIP BLIP BLIP2 ALBUF

## 5. Leaf-wise和Level-wise
- Level-wise  广度优先构建树。每一层的所有节点同时分裂，再进入下一层, 常见于：XGBoost, Catboost的也类似这种
- Leaf-wise  每次只选择 一个最优叶子节点 来分裂 —— 通常是“带来最大信息增益（Gain）”的那个。 常见于：LightGBM

如果你对过拟合非常敏感（如样本量小），Level-wise 更稳健。
如果你追求极致性能（如大样本、稀疏特征、低噪声），Leaf-wise 更高效、更准确，但要小心过拟合，需调好 max_depth 和 min_data_in_leaf。

